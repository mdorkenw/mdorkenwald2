<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script> -->
    <!-- <script type="text/javascript" src="js/hidebib.js"></script> -->
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-7580334-2');
    </script>
    <title>Michael Dorkenwald</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">

    <meta name="author" content="Michael Dorkenwald">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="stylesheet" type="text/css" href="css/style.css">
    <link rel="icon" type="image/png" href="images/JHU_icon.jpg">
</head>

<body>
    <table style="width:100%; max-width:800px; border:0; border-spacing:0; border-collapse:separate; margin:auto;">
        <tr style="padding:0">
            <td style="padding:0">
                <table style="width:100%; border:0; border-spacing:0; border-collapse:separate; margin:auto;">
                    <tr style="padding:0">
                        <!-- Adjusted width from 80% to 70% to allocate space for the avatar -->
                        <td style="padding:2.5%; width:70%; vertical-align:middle;">
                            <p style="text-align:center;">
                                <name>Michael Dorkenwald</name>
                            </p>
                            <!-- Content Body -->
                            <div>
                                <p>I am a PhD student in the <a href="https://ivi.fnwi.uva.nl/quva/">QUVA lab</a> at the <a href="https://www.uva.nl/en">University of Amsterdam</a> supervised by <a href="https://yukimasano.github.io/">Yuki Asano</a> and <a href="https://www.ceessnoek.info/index.php/biography/">Cees Snoek</a>. I am also part of the <a href="https://ellis.eu/projects/generalizeable-video-representation-learning">ELLIS PhD program</a> in cooperation with Qualcomm.</p>

                                <p>Before, I received my master's degree in physics from <a href="https://www.uni-heidelberg.de/en">Heidelberg University</a> during which I was part of the research group from <a href="https://hci.iwr.uni-heidelberg.de/people/bommer">Bj√∂rn Ommer</a>. There, I was working on understanding human and object dynamics within generative frameworks primarily for video synthesis. I had the opportunity for a research visit at <a href="https://www.cs.ryerson.ca/kosta/">Kosta Derpanis's</a> lab in Toronto. Furthermore, I completed an internship in the AWS Rekognition team where I worked on self-supervised video representation learning.</p>
                            </div>
                            <!-- Contact Links -->
                            <p style="text-align:center;">
                                <a href="mailto:m.dorkenwald@gmail.com">Email</a> &nbsp;/&nbsp;
                                <a href="https://scholar.google.com/citations?user=KY5nvLUAAAAJ&hl=de&oi=ao">Google Scholar</a> &nbsp;/&nbsp;
                                <a href="https://twitter.com/mdorkenw">Twitter</a> &nbsp;/&nbsp;
                                <a href="https://github.com/mdorkenw">Github</a> &nbsp;/&nbsp;
                                <a href="https://www.linkedin.com/in/michael-dorkenwald-29303b182">LinkedIn</a>
                            </p>
                        </td>
                        <!-- Adjusted width to 30% and added vertical-align and text-align -->
                        <td style="padding:1%; width:24%; vertical-align:middle; text-align:center;">
                            <a href="images/avatar.jpg">
                                <img
                                    src="images/avatar.jpg"
                                    alt="profile photo"
                                    style="width:100%; border-radius:50%; object-fit:cover;"
                                >
                            </a>
                        </td>



          </tr>
        </tbody>
      </table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading><font color="black">News</font></heading>
              <ul>
                <li>[Sep 2025] One paper accepted to NeurIPS'25 on elastic pruning of ViTs. Paper will be out soon.</li>
                <li>[Jul 2025] One paper accepted to BMVC'25 on introducing a new a <a href="https://arxiv.org/abs/2410.07752">video-language benchmark</a>.</li>
                <li>[Jun 2025] One paper accepted to TMLR on  <a href="https://openreview.net/forum?id=kZLANTp6Vw&nesting=2&sort=date-desc">LLMs as implicit optimizers for VLMs</a>.</li>
                <li>[Nov 2024] Gave a talk at the <a href="https://communities.surf.nl/large-scale-computing/artikel/surf-research-bootcamp-2024">Surf research bootcamp</a> on large scale video learning.</li>
                <li>[Sep 2024] Workshop organizer of <a href="https://sslwin.org/">"Self Supervised Learning: What is Next?"</a> at ECCV'24.</li>
                <li>[Jul 2024] One paper accepted to ECCV'24 on <a href="https://quva-lab.github.io/SIGMA/">masked video modeling.</a></li>
                <li>[Jun 2024] Gave a talk at TNO in den Hague and at the National Institute for Informatics in Tokyo.</li>
                <li>[Apr 2024] Teaching Assistant for the <a href="https://uvafomo.github.io/">Foundation Models (FoMo)</a> course.</li>
                <li>[Feb 2024] One paper accepted to CVPR'24 on <a href="https://quva-lab.github.io/PIN/">enabling object localisation abilities in VLMs.</a></li>
                <li>[Jul 2023] Attended the <a href="https://iplab.dmi.unict.it/icvss2023/Home">International Computer Vision Summer School</a> in Sicily.</li>
            </ul>
            </td>
          </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research focuses on the intersection of self-supervised learning, video understanding, and vision-language models.
              </p>
            </td>
          </tr>
        </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td class="tdimg">
                  <img src='images/tvbench.png'>
                </td>
                <td class="tdcontent" style="padding:20px;width:65%;vertical-align:center", bgcolor="transparent">
                  <p>
                    <a href="https://arxiv.org/abs/2410.07752">
                      <papertitle>TVBench: Redesigning Video-Language Evaluation</papertitle>
                    </a>
                    <br>
                  Daniel Cores*, <strong>Michael Dorkenwald*</strong>, Manuel Mucientes, Cees G. M. Snoek, Yuki M. Asano<br>
                  <em>Accepted to BMVC 2025</em>
                  <br>
                  </p>
                  <div class="paper" id="">
                    <a href="https://arxiv.org/abs/2410.07752">ArXiv</a> &nbsp/&nbsp
                    <!-- <a href="https://quva-lab.github.io/SIGMA/">Project Page</a> &nbsp/&nbsp -->
                    <a href="https://github.com/daniel-cores/tvbench">Code </a>  &nbsp/&nbsp
                    <a href="https://huggingface.co/datasets/FunAILab/TVBench">Hugging Face</a>
                  </div>
    
                </td>
              </tr>

              <tr>
                <td class="tdimg">
                  <img src='images/glov.png'>
                </td>
                <td class="tdcontent" style="padding:20px;width:65%;vertical-align:center", bgcolor="transparent">
                  <p>
                    <a href="https://arxiv.org/abs/2410.06154">
                      <papertitle>GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models</papertitle>
                    </a>
                    <br>
                    M. Jehanzeb Mirza, Mengjie Zhao, Zhuoyuan Mao, Sivan Doveh, Wei Lin, Paul Gavrikov, <strong>Michael Dorkenwald</strong>, Shiqi Yang, Saurav Jha, Hiromi Wakaki, Yuki Mitsufuji, Horst Possegger, Rogerio Feris, Leonid Karlinsky, James Glass
                    <br>
                    <em>TMLR</em>
                  </p>
                  <div class="paper" id="">
                    <a href="https://arxiv.org/abs/2410.06154">ArXiv</a> &nbsp/&nbsp
                    <a href="https://jmiemirza.github.io/GLOV/">Project Page</a> &nbsp/&nbsp
                    <a href="https://github.com/jmiemirza/GLOV">Code</a>
                  </div>
    
                </td>
              </tr>
          <tr>
            <td class="tdimg">
              <img src='images/sigma.png'>
            </td>
            <td class="tdcontent" style="padding:20px;width:65%;vertical-align:center", bgcolor="transparent">
              <p>
                <a href="https://arxiv.org/abs/2407.15447">
                  <papertitle>SIGMA: Sinkhorn-Guided Masked Video Modeling</papertitle>
                </a>
                <br>
                Mohammadreza Salehi*,  <strong>Michael Dorkenwald*</strong>, Fida Mohammad Thoker*, Efstratios Gavves, Cees Snoek, Yuki M. Asano<br>
                <em>ECCV 2024</em>
                <br>
              </p>
              <div class="paper" id="">
                <a href="https://arxiv.org/abs/2407.15447">ArXiv</a> &nbsp/&nbsp
                <a href="https://quva-lab.github.io/SIGMA/">Project Page</a> &nbsp/&nbsp
                <a href="https://github.com/QUVA-Lab/SIGMA">Code</a>
              </div>

            </td>
          </tr>

          <tr>
            <td class="tdimg">
              <img src='images/pin.jpeg'>
            </td>
            <td class="tdcontent" style="padding:20px;width:65%;vertical-align:center", bgcolor="transparent">
              <p>
                <a href="https://arxiv.org/abs/2402.08657">
                  <papertitle>PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs</papertitle>
                </a>
                <br>
                <strong>Michael Dorkenwald</strong>, Nimrod Barazani, Cees Snoek*, Yuki M. Asano*
                <br>
                <em>CVPR 2024</em>
                <br>
              </p>
              <div class="paper" id="">
                <a href="https://arxiv.org/abs/2402.08657">ArXiv</a> &nbsp/&nbsp
                <a href="https://quva-lab.github.io/PIN/">Project Page</a> &nbsp/&nbsp
                <a href="https://github.com/QUVA-Lab/PIN/">Code</a>
              </div>

            </td>
          </tr>


          <tr>
            <td class="tdimg">
                <img src='images/scvrl.png' alt="Image">
              </td>
            <td class="tdcontent" style="padding:20px;width:65%;vertical-align:center;background-color:transparent;">
              <p>
                <a href="https://arxiv.org/abs/2205.11710">
                  <papertitle>SCVRL: Shuffled Contrastive Video Representation Learning</papertitle>
                </a>
                <br>
                <strong>Michael Dorkenwald</strong>, Fanyi Xiao, Biagio Brattoli, Joseph Tighe, Davide Modolo
                <br>
                <em>CVPR 2022 I3D-IVU workshop</em>
                <br>
              </p>
              <div class="paper" id="">
                <a href="https://arxiv.org/abs/2205.11710">ArXiv</a> &nbsp/&nbsp
                <a href="https://www.amazon.science/publications/scvrl-shuffled-contrastive-video-representation-learning">Project Page</a>
              </div>
            </td>
          </tr>
          
          <tr>
            <td class="tdimg">
              <img src='images/ipoke.png' alt="Image">
            </td>
            <td class="tdcontent" style="padding:20px;width:65%;vertical-align:center;background-color:transparent;">
                <div class="content">
                <p>
                  <a href="https://arxiv.org/abs/2107.02790">
                    <papertitle>iPOKE: Poking a Still Image for Controlled Stochastic Video Synthesis</papertitle>
                  </a>
                  <br>
                  Andreas Blattmann, Timo Milbich, <strong>Michael Dorkenwald</strong>, Bj√∂rn Ommer
                  <br>
                  <em>ICCV 2021</em>
                  <br>
                </p>
                <div class="paper" id="">
                  <a href="https://arxiv.org/abs/2107.02790">ArXiv</a> &nbsp;/&nbsp;
                  <a href="https://compvis.github.io/ipoke/">Project Page</a> &nbsp;/&nbsp;
                  <a href="https://github.com/CompVis/ipoke">Code</a>
                </div>
              </div>
            </td>
          </tr>

          <tr>
            <td class="tdimg">
              <img src='images/i2v.png'>
            </td>
            <td class="tdcontent" style="padding:20px;width:65%;vertical-align:center;background-color:transparent;">
              <p>
                <a href="https://arxiv.org/abs/2105.04551">
                  <papertitle>Stochastic Image-to-Video Synthesis using cINNs</papertitle>
                </a>
                <br>
                <strong>Michael Dorkenwald</strong>, Timo Milbich, Andreas Blattmann, Robin Rombach, Konstantinos G. Derpanis, Bj√∂rn Ommer
                <br>
                <em>CVPR 2021</em>
                <br>
              </p>
              <div class="paper" id="">
                <a href="https://arxiv.org/abs/2105.04551">ArXiv</a> &nbsp/&nbsp
                <a href="https://compvis.github.io/image2video-synthesis-using-cINNs/">Project Page</a> &nbsp/&nbsp
                <a href="https://github.com/CompVis/image2video-synthesis-using-cINNs">Code</a>
              </div>
            </td>
          </tr>
          
          <tr>
            <td class="tdimg">
              <img src='images/behavior.png' alt="Image">
            </td>
            <td class="tdcontent" style="padding:20px;width:65%;vertical-align:center;background-color:transparent;">
                <div class="content">
                <p>
                  <a href="https://arxiv.org/abs/2103.04677">
                    <papertitle>Behavior-Driven Synthesis of Human Dynamics</papertitle>
                  </a>
                  <br>
                  Andreas Blattmann, Timo Milbich, <strong>Michael Dorkenwald</strong>, Bj√∂rn Ommer
                  <br>
                  <em>CVPR 2021</em>
                  <br>
                </p>
                <div class="paper" id="">
                  <a href="https://arxiv.org/abs/2103.04677">ArXiv</a> &nbsp;/&nbsp;
                  <a href="https://compvis.github.io/behavior-driven-video-synthesis/">Project Page</a> &nbsp;/&nbsp;
                  <a href="https://github.com/CompVis/behavior-driven-video-synthesis">Code</a>
                </div>
              </div>
            </td>
          </tr>

          <tr>
            <td class="tdimg">
              <img src='images/andy_cvpr.png' alt="Image">
            </td>
            <td class="tdcontent" style="padding:20px;width:65%;vertical-align:center;background-color:transparent;">
                <div class="content">
                <p>
                  <a href="https://arxiv.org/abs/2106.11303">
                    <papertitle>Understanding Object Dynamics for Interactive Image-to-Video Synthesis</papertitle>
                  </a>
                  <br>
                  Andreas Blattmann, Timo Milbich, <strong>Michael Dorkenwald</strong>, Bj√∂rn Ommer
                  <br>
                  <em>CVPR 2021</em>
                  <br>
                </p>
                <div class="paper" id="">
                  <a href="https://arxiv.org/abs/2106.11303">ArXiv</a> &nbsp;/&nbsp;
                  <a href="https://compvis.github.io/interactive-image2video-synthesis/">Project Page</a> &nbsp;/&nbsp;
                  <a href="https://github.com/CompVis/interactive-image2video-synthesis">Code</a>
                </div>
              </div>
            </td>
          </tr>

          <tr>
            <td class="tdimg">
              <img src='images/ubam.png' alt="Image">
            </td>
            <td class="tdcontent" style="padding:20px;width:65%;vertical-align:center;background-color:transparent;">
                <div class="content">
                <p>
                  <a href="https://rdcu.be/ch6pL">
                    <papertitle>Unsupervised behaviour analysis and magnification (uBAM) using deep learning</papertitle>
                  </a>
                  <br>
                  Biagio Brattoli*, Uta Buechler*, <strong>Michael Dorkenwald</strong>, Philipp Reiser, Lineard Filli, Fritjof Helmchen, Anna-Sophia Wahl, Bj√∂rn Ommer
                  <br>
                  <em>Nature Machine Intelligence</em>
                  <br>
                </p>
                <div class="paper" id="">
                  <a href="https://rdcu.be/ch6pL">Article</a> &nbsp;/&nbsp;
                  <a href="https://utabuechler.github.io/behaviourAnalysis/index.html">Project Page</a> &nbsp;/&nbsp;
                  <a href="https://github.com/utabuechler/uBAM">Code</a>
                </div>
              </div>
            </td>
          </tr>
          
          <tr>
            <td class="tdimg">
              <img src='images/magnification.png' alt="Image">
            </td>
            <td class="tdcontent" style="padding:20px;width:65%;vertical-align:center;background-color:transparent;">
                <div class="content">
                <p>
                  <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Dorkenwald_Unsupervised_Magnification_of_Posture_Deviations_Across_Subjects_CVPR_2020_paper.pdf">
                    <papertitle>Unsupervised Magnification of Posture Deviations across Subjects</papertitle>
                  </a>
                  <br>
                  <strong>Michael Dorkenwald*</strong>, Uta B√ºchler*, Bj√∂rn Ommer
                  <br>
                  <em>CVPR 2020</em>
                  <br>
                </p>
                <div class="paper" id="">
                  <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Dorkenwald_Unsupervised_Magnification_of_Posture_Deviations_Across_Subjects_CVPR_2020_paper.pdf">Article</a> &nbsp;/&nbsp;
                  <a href="https://compvis.github.io/magnify-posture-deviations/">Project Page</a>
                </div>
              </div>
            </td>
          </tr>

  </table>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

<!-- Google Analytics -->
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-FMGWZGV2XD"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-FMGWZGV2XD');
</script>
<!-- End Google Analytics -->

</body>

</html>
